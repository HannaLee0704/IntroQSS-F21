---
title: "6.1 Regression intuition"
subtitle: "PLSC30500, Fall 2021"
output: 
  xaringan::moon_reader:
    css: [default, fc, fc-fonts]
    ration: '16:9'
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
  
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align = "center", out.width = "900px", fig.width = 4.5, fig.asp = .7)
```


class: inverse, middle, center

# Two key facts about regression 


<!-- Todo: might add a thing about interpreting the coefficient by plotting it via predictions, etc -- what IS the best way to do that, anyway? might be sjplot plot_model  -->

---

name: fact-1-task


## Fact 1: slope from covariance and variance


Slope coefficient from bivariate regression of $Y$ on $X$: 

$$  \hat{\beta} = \frac{\text{cov}(Y, X)}{\text{var}(X)}$$
--

**Task 1**: demonstrate this using `mtcars` (or any other dataset!)

See [solution](#fact-1-task-solution).

???

Fun fact: any OLS coefficient can be expressed in terms of covariances and variances, not just a bivariate regression slope.


---

name: fact-2-task

## Fact 2: residuals uncorrelated with regressors

If $X$ is a regressor (i.e. predictor, right-hand-side (RHS) variable) and $\hat{r}$ is the vector of residuals, then 
$$ \text{cov}(X, \hat{r}) = 0 $$

--

**Task 2:** Confirm this with `mtcars`. (See [solution](#fact-2-task-solution))

--

<br/><br/> 

<br/><br/> 

**Reminder:** If `my_lm` is your regression object, you can get the residuals with 

- `resid(my_lm)`, 
- `my_lm$residuals`, or 
- `my_lm %>% broom::augment()`.


---

## Fact 2 intuition (1)

```{r ols-not-ols, echo = F}
mtcars_reg <- lm(mpg ~ wt, data = mtcars)
bad_intercept <- coef(mtcars_reg)[1]*.875
bad_slope <- coef(mtcars_reg)[2]*.75

mtcars %>%
  ggplot(aes(x = wt, y = mpg)) + 
  geom_point() + 
  # the OLS line
  geom_smooth(method = lm, formula = y ~ x,
              col = "blue", se = F) + 
  annotate(x = 4.5, y = 11, geom = "text", label = "OLS", 
           col = "blue", size = 3) +
  # the not OLS line
  geom_abline(intercept = bad_intercept,
         slope = bad_slope, col = "red") + 
  annotate(x = 4.5, y = 18, geom = "text", label = "Not OLS",
           col = "red", size = 3)
```

???

We do two prediction lines: the OLS one and a "bad" one

---


## Fact 2 intuition (2)

```{r fact-2-intuition-2, echo = F}
mtcars %>% 
  mutate(`1. OLS` = coef(mtcars_reg)[1] + coef(mtcars_reg)[2]*wt,
         `2. Not OLS` = bad_intercept + bad_slope*wt) %>% 
  pivot_longer(cols = matches("OLS"), values_to = "prediction", names_to = "prediction_type") %>% 
  mutate(Residuals = mpg - prediction) %>% 
  ggplot(aes(x = wt, y = Residuals)) + 
  geom_point() + 
  geom_smooth(aes(col = prediction_type),
              method = "lm", formula = y ~ x, se = F,
              show.legend = F) + 
  scale_color_manual(values = c("blue", "red")) +
  facet_wrap(. ~ prediction_type)
```


???

The residuals from the bad prediction are correlated with `wt`, but not the residuals from the OLS prediction. 

If the residuals are correlated with a predictor (as in the "Not OLS" prediction line), we can make the prediction better by shifting the line and getting rid of that correlation. When we do that for all predictors we get the OLS solution. 



---

class: inverse, middle, center

# Using the two facts (1):
# the Frisch-Waugh-Lovell (FWL) theorem

---

# FWL theorem and interpreting regression coefficients

Suppose we regress $Y$ on $D$ ("treatment") and $X$ ("covariate"). What does the coefficient on $D$ mean?

--

name: fwl-task

The **Frisch-Waugh-Lovell (FWL) Theorem** says that these produce the same number: 

- regress $Y$ on $D$ and $X$, get coefficient on $D$
- regress $D$ on $X$, get residuals; regress $Y$ on those residuals, get coefficient

--

<br> </br>

**Task:** confirm FWL in `mtcars`. 
<!-- (use `mpg` as $Y$, `wt` as $D$, and `disp` as $X$).-->
(See [solution](fwl-task-solution))

???

This also works when you have many covariates $X_1, X_2, \ldots$, not just one



---


###  The math behind the FWL theorem

Consider three fitted regression equations:

$$\begin{eqnarray}
Y_i &=& \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i + \hat{r}_i  \\
D_i &=& \hat{\alpha}_0 + \hat{\alpha}_1 X_i  + \hat{\varepsilon}_i \\
Y_i &=& \hat{\gamma}_0 + \hat{\gamma}_1 \hat{\varepsilon}_i + \hat{u}_i
\end{eqnarray}$$
FWL says $\hat{\beta}_1 = \hat{\gamma}_1$.

**Proof**: $\hat{\gamma}_1$ can be written (fact 1) $\frac{\text{cov}(Y_i, \hat{\varepsilon}_i)}{\text{var}(\hat{\varepsilon}_i)} =\frac{\text{cov}(\hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i + \hat{r}_i, \hat{\varepsilon}_i)}{\text{var}(\hat{\varepsilon}_i)}$

Now observe that

- $\hat{\beta}_0$ is a constant $\rightarrow$ not correlated w. $\hat{\varepsilon}_i$
- $X_i$ is uncorrelated with $\hat{\varepsilon}$ (**fact 2**), and
- $\hat{r}_i$ is uncorrelated with $\hat{\varepsilon}$ b/c $D_i$ is uncorrelated with $\hat{r}_i$ (**fact 2**), and $\hat{\varepsilon}_i$ is part of $D_i$.

So:

$$\begin{eqnarray} \frac{\text{cov}(Y_i, \hat{\varepsilon}_i)}{\text{var}(\hat{\varepsilon}_i)} = \frac{\text{cov}(\hat{\beta_1} D_i, \hat{\varepsilon}_i)}{\text{var}(\hat{\varepsilon}_i)} =
\hat{\beta_1} \frac{\text{cov}(\hat{\alpha}_0 + \hat{\alpha}_1 X_i  + \hat{\varepsilon}_i, \hat{\varepsilon}_i)}{\text{var}(\hat{\varepsilon}_i)} =
\hat{\beta_1}\frac{\text{cov}(\hat{\varepsilon}_i, \hat{\varepsilon}_i)}{\text{var}(\hat{\varepsilon}_i)} = \hat{\beta_1}
\end{eqnarray}$$



---

## Using the FWL theorem

To interpret $\hat{\beta}_1$ in a regression like 
$$
Y_i = \hat{\beta}_0 + \hat{\beta_1} D_i + \hat{\beta_2} X_i,
$$ 
think of FWL: $\hat{\beta}_1$ describes the relationship between $Y_i$ and the part of $D_i$ that is not "explained" by $X_i$. 

--

FWL says we can learn about a regression coefficient using a **partial regression plot**.  

???

"Explained" is in quotes here because regressing $D$ on $X$ does not explain any part of $D$ in a deep sense; OLS is just describing their relationship in the dataset. 

---

#### Partial regression plot

```{r, out.width = "550px"}
mtcars %>%
  mutate(wt_resid = resid(lm(wt ~ disp, data = .))) %>%
  ggplot(aes(x = wt_resid, y = mpg)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = y ~ x, se = F) + 
  labs(x = "Residualized vehicle weight\n(adjusted for displacement)", 
       y = "Vehicle MPG")
```

---

class: center, inverse, middle

# Using the two facts (2):
# the Omitted variable bias (OVB) equation

---
name: ovb-task

### OVB equation and interpreting regression coefficients 

Suppose we have regressed $Y$ on $D$ ("treatment"). How would our coefficient change if we included $X$ in the regression? 

--

OVB equation says that the difference between the first and second coefficient is the product of  

- the coefficient on $X$ in the full regression ("impact")
- the coefficient on $D$ from a regression of $X$ on $D$ ("imbalance")

--

Confirm that this is true with `mtcars`. <!-- (use `mpg` as $Y$, `wt` as $D$, and `disp` as $X$).-->  (See [solution](#ovb-task-solution).)

???

This is a fact about regression. It only makes sense to call the difference in the two estimates "bias" if the longer equation is correct, but the fact is true whether or not the longer equation is correct.

The question here is one you should ask when thinking about any empirical finding: how would the results be different if the regression adjusted for an additional factor? 

---



### The math behind the OVB equation 

Consider three fitted regression equations: 

$$\begin{eqnarray} 
Y_i &=& \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i + \hat{r}_i \\
Y_i &=& \hat{\gamma}_0 + \hat{\gamma}_1 D_i + \hat{\varepsilon}_i \\
X_i &=& \hat{\alpha}_0 + \hat{\alpha}_1 D_i + \hat{u}_i
\end{eqnarray}$$

OVB equation says: $\hat{\gamma}_1 = \hat{\beta}_1 + \hat{\beta}_2 \hat{\alpha}_1$. 

--

**Proof**: Using **fact 1** we have
$$ \hat{\gamma}_1 = \frac{\text{cov}(Y_i, D_i)}{\text{var}(D_i)} = \frac{\text{cov}(\hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i + \hat{r}_i, D_i)}{\text{var}(D_i)} $$ 

and using fact 2 this becomes  

$$ \hat{\beta}_1 \frac{\text{cov}(D_i, D_i)}{\text{var}(D_i)} + \hat{\beta}_2 \frac{\text{cov}(X_i, D_i)}{\text{var}(D_i)} = \hat{\beta}_1 + \hat{\beta}_2 \frac{\text{cov}(X_i, D_i)}{\text{var}(D_i)}$$
which (using fact 1) is $\hat{\beta}_1 + \hat{\beta}_2 \times \hat{\alpha}_1$.

???

The $\hat{\beta}_2 \hat{\alpha}_1$ term is impact $\times$ imabalance, and this is "bias" if the long model is correct so that $\beta_1$ is the target. But leaving aside the "bias" interpretation, this fact about regression doesn't depend on the "long" regression being correct (though sometimes it's presented that way) or on onything being causal. 

---

### OVB practice 


Suppose we regress a measure of civil conflict on an indicator for whether peacekeeping had taken place in the previous 5 years.

--

How is our estimate different from the estimate we would get if we adjusted for previous level of civil conflict? 

<br> </br>
<br> </br>


--

"Impact" here is: coefficient on previous civil conflict in regression of civil conflict on peacekeeping 

"Imbalance" here is: difference in previous civil conflict between places with vs. without peacekeeping. 

???

Impact and imbalance in this example would probably both be positive, so the "bias" is positive, i.e. we get a larger coefficient without adjusting for previous conflict than we would if we did adjust for it. 

---

class: center, inverse, middle

## Task solutions

---

name: fact-1-task-solution

#### Solution to task 1 (fact 1 demo) 

.pull-left[
```{r plot-right, fig.show = "hide"}
mtcars %>% 
  ggplot(aes(x = wt, 
             y = mpg)) + 
  geom_point()

```
]

.pull-right[
```{r ref.label = "plot-right", echo = F, fig.width = 3, out.width="450px"}
```
]

```{r}
lm(mpg ~ wt, data = mtcars) %>% coef()
```

```{r}
cov(mtcars$mpg, mtcars$wt)/var(mtcars$wt) 
```

Back to [task 1](#fact-1-task).


---

name: fact-2-task-solution

#### Solutions to task 2 (fact 2)
<!-- ## Confirming fact 2: choose your style  -->

```{r}
#### Explicit
my_lm <- lm(mpg ~ wt, data = mtcars)
my_resids <- my_lm$residuals
cov(mtcars$wt, my_resids)
```

```{r}
#### One-liner/"code golf"
cov(mtcars$wt, resid(lm(mpg ~ wt, data = mtcars)))
```

```{r}
#### Tidy fanatic
mtcars %>% 
  lm(mpg ~ wt, data = .) %>% 
  broom::augment() %>% 
  summarize(cov(wt, .resid)) %>% 
  as.numeric()
```

(Back to [task 2](#fact-2-task))

???

That's as close to zero as the computer knows how to get.

---
name: fwl-task-solution

#### Confirming FWL 

```{r}
lm(mpg ~ wt + hp, data = mtcars) %>% coef()

# tidy-ish way
mtcars %>% 
  mutate(resid = resid(lm(wt ~ hp, data = .))) %>% 
  lm(mpg ~ resid, data = .) %>% 
  coef()

# more straightforward way
resids <- resid(lm(wt ~ hp, data = mtcars))
lm(mtcars$mpg ~ resids) %>% coef()

```

(Back to [FWL](fwl-task).)

--- 
name: ovb-task-solution

#### Confirming OVB equation


```{r ovb-task}
long_lm <- lm(mpg ~ wt + disp, data = mtcars)
impact <- coef(long_lm)["disp"]
imbalance <- coef(lm(disp ~ wt, data = mtcars))["wt"]
short_lm <- lm(mpg ~ wt, data = mtcars)

coef(long_lm)["wt"] - coef(short_lm)["wt"]
-impact*imbalance

```

([Back to OVB task](#ovb-task).)

---

class: inverse, middle, center

## Other omitted code 

---

name: ols-not-ols-code

```{r ref.label = "ols-not-ols", eval = T, out.width = "350px"}
```


---

name: fact-2-intuition-2-code

```{r ref.label = "fact-2-intuition-2", eval = T, out.width = "350px"}
```

