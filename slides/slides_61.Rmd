---
title: "PLSC30500, Fall 2021"
subtitle: "6.1 Regression for causal inference"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align = "center", out.width = "900px", fig.width = 4.5, fig.asp = .7)
```

```{css, echo = F}

.small-output .remark-code{
  font-size: small;
}
```



## Conditional average treatment effect (review)

Recall Broockman & Butler experiment from lecture 4.2.

We considered *conditional average treatment effects*, e.g. 

$$E[Y_i(1) - Y_i(0) | \mathrm{party} = \mathrm{Dem}]$$
--

Randomization means treatment $D_i$ is independent of potential outcomes $Y_i(0), Y_i(1)$. 

--

So the *conditional difference in means* is an unbiased estimator of this and other CATEs: 

\begin{align*}
E[Y_i(1) - Y_i(0) | X_i = x] &= \underbrace{E[Y_i(1) | X_i = x ]}_{\text{Avg. }Y_i(1)\text{ for units with }X_i = x} - \underbrace{E[Y_i(0) | X_i = x]}_{\text{Avg. }Y_i(0)\text{ for units with }X_i = x} \\
&= E[Y_i(1) | D_i = 1, X_i = x ] - E[Y_i(0) | D_i = 0, X_i = x] \\
&= \underbrace{E[Y_i | D_i = 1, X_i = x ]}_{\text{Avg. }Y_i\text{ for units with }D_1 = 1, X_i = x} - \underbrace{E[Y_i | D_i = 0, X_i = x]}_{\text{Avg. }Y_i\text{ for units with }D_1 = 0, X_i = x} \\
\end{align*}

???

To go from 1 to 2: $D_i$ independent of $Y_i(0),Y_i(1)$
To go from 2 to 3: potential outcomes model, i.e. $Y_i = D_i Y_i(1) + (1 - D_i)Y_i(0)$

---

## Conditional ignorability

In some cases, independence of potential outcomes and treatment

$$\left(Y_i(1), Y_i(0)\right) \perp\!\!\!\!\perp  D_i$$ 
may not hold *in general*, but it may hold *conditional on some* $X_i$: 

$$\left(Y_i(1), Y_i(0)\right) \perp\!\!\!\!\perp  D_i \mid X_i.$$ 
Then $D_i$ is *strongly ignorable* conditional on $X_i$.

--

**Example**: Suppose in Broockman & Butler the randomization went like this:

\begin{align*}
\text{Pr}[\text{DeShawn email} | \text{Dem legislator}] &= .4 \\
\text{Pr}[\text{DeShawn email} | \text{Rep legislator}] &= .6 \\
\end{align*}

--

Then treatment is related to party (which is probably related to potential outcomes), but unrelated to potential outcomes *conditional on* party. 

???

It is called *ignorability* because you can ignore the way that treatment was assigned.

$\left(Y_i(1), Y_i(0)\right) \perp\!\!\!\!\perp  D_i \mid X_i$ is the conditional independence assumption. Technically you also need positivity: $0 < \text{Pr}[D_i = 1 | X_i] < 1$. 

**Strong** ignorability relates to the pair $Y_i(0), Y_i(1)$. **Weak** ignorability in the binary treatment case says $Y_i(1) \perp\!\!\!\!\perp  D_i \mid X_i$ and $Y_i(0) \perp\!\!\!\!\perp  D_i \mid X_i$, so it permits a relationship between the treatment effect and $D_i$ conditional on $X_i$. But note that the key results would work with weak ignorability.  

---

## Another example of conditional ignorability 

Recall our example of the effect of studying on final grades:

- AB types: $Y(1) = 95$, $Y(0) = 85$
- BC types: $Y(1) = 85$, $Y(0) = 75$

--

Suppose most AB types study and most BC types don't. 

--

Then overall difference-in-means will be biased (which way?).

--

But treatment is strongly ignorable conditional on type (because potential outcomes identical within types).

---

## Conditional ignorability and the CATE

If treatment is strongly ignorable conditional on $X_i$, then treatment is *as-if random* within levels of $X_i$.

--

$\implies$ the conditional difference in means is an unbiased estimator of the conditional average treatment effect, $\mathrm{CATE}$.

--

So 

| Study | Difference in means unbiased for ATE? | Conditional difference in means unbiased for CATE? | 
|:---:|:---:| :----:|
|Original Broockman and Butler | Yes | Yes |
|Modified Broockman and Butler | No | Yes |

---

## From conditional ignorability to the ATE

Wait: if you have an unbiased estimator for the conditional ATE at each value of $X_i$, don't you have an unbiased estimator for the ATE? 

--

For example: 

- to estimate the ATE of the "DeShawn" email on response rates, take the difference in means separately for Dem and Rep legislators, and then combine them
- to estimate the ATE of studying on academic performance, take the difference in means separately for AB types and BC types, and then combine them
- to estimate ATE of peacekeeping on civil conflict, take the difference in means for types of countries (e.g. low previous conflict, medium previous conflict) and then combine them

--

This is what we mean by "controlling for $X$" or "conditioning on $X$" to get the ATE.

---

## From conditional ignorability to the ATE (more formally)

By the *Law of Iterated Expectations (LIE)*,

\begin{align*}
\text{ATE} &\equiv E[\tau_i] \\
&= \sum_x E[\tau_i | X_i = x] \text{Pr}[X_i = x]
\end{align*}

--


Other examples of LIE:

- to compute average age in our class, can compute average age for men and average age for women, then combine by share of men/women 
- to compute world average GDP/capita, compute for each country separately, then combine by population shares 

--

So if strong ignorability holds, an unbiased estimator for ATE is the weighted average of differences in means.

---

## The math (if it helps)

\begin{align*}
\text{ATE} &\equiv E[\tau_i] \\ 
&= \sum_x E[\tau_i | X_i = x] \mathrm{Pr}[X_i = x]  \\
&= \sum_x E[Y_i(1) -  Y_i(0)| X_i = x] \mathrm{Pr}[X_i = x] \\
&= \sum_x \left( E[Y_i(1) | X_i = x] - E[Y_i(0)| X_i = x] \right) \mathrm{Pr}[X_i = x]  \\
&= \sum_x \left( E[Y_i(1) | D_i = 1, X_i = x] - E[Y_i(0)| D_i = 0, X_i = x] \right) \mathrm{Pr}[X_i = x] \\
&= \sum_x \left( E[Y_i | D_i = 1, X_i = x] - E[Y_i| D_i = 0, X_i = x] \right) \mathrm{Pr}[X_i = x]  
\end{align*}

Note:

- 1 is by definition of ATE
- 2 is by law of iterated expectations
- 3 is by definition of ITE
- 4 is by linearity of expectations
- 5 is by assumption of potential outcomes model (SUTVA)
- 6 is by strong ignorability

See Theorem 7.1.13 in Aronow and Miller.

---

## Back to regression

Regression provides a convenient way to compute and combine differences in means conditional on covariates $X_1$, $X_2$, etc.

--

For example, the regression equation

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\tau} D_i + \hat{\beta}_1 X_{i1} + \hat{\beta}_2 X_{i2}$$ 

lets us calculate conditional differences in (predicted) means 

$$\hat{E}[Y_i | D_i = 1, X_{i1} = x_1, X_{i2} = x_2] - \hat{E}[Y_i | D_i = 0, X_{i1} = x_1, X_{i2} = x_2] = \hat{\tau}.$$ 

--

If conditional ignorability holds, this is an unbiased estimate of a CATE.

--

The regression equation above assumes CATEs are all the same; if so, this regression gives unbiased estimate of ATE; if not, it gives a weighted average of CATEs.

???

If conditional ignorability holds and you have categorical data with a saturdated model but no interactions with treatment, then I think the coefficient on treatment is a weighted average of the CATEs, where weights are according to variance of treatment in each cell. See Mostly Harmless and Matt Blackwell's notes.

---

## Regression and other predictive methods

We have described regression (OLS) as a way to predict $Y$ as a function of $D_i$, $X_i$, etc.

--

Given conditional ignorability, this can yield unbiased estimates of the ATE. 

--

There are other ways to predict $Y$ as a function $D_i$, $X_i$, etc, including: 

- maximum likelihood estimation (MLE)
- matching
- weighting
- machine learning methods (e.g. random forests, neural nets)

Take more courses to find out more!

---

## Conditional ignorability and DAGs

Recall: "treatment is ignorable conditional on $\mathbf{X}_i$" means 

$(Y_i(0), Y_i(1))  \perp\!\!\!\!\perp D_i | \mathbf{X}_i$

--

The DAG version of this that that the only unblocked path from $D$ to $Y$ is the treatment effect of interest.

```{r, fig.height=3, fig.width = 4, fig.align = "center", echo = F, message = F, out.width = "80%"}
library(ggdag)
coords <- tibble::tribble(
  ~name, ~x,  ~y,
  "D", 0,   0,
  "X", -.25,   .25,
  "Y",  1,   0,
  "W", .5, -.25
)

dagify(Y ~ X + D,
       D ~ X,
       W ~ D,
       W ~ Y,
       coords = coords)%>% 
  tidy_dagitty() %>% 
  ggdag() + 
  theme_void()

```


---

## Another DAG

```{r, fig.height=3, fig.width = 4, fig.align = "center", echo = F, message = F, out.width = "80%"}
coords <- tibble::tribble(
  ~name, ~x,  ~y,
  "D", 0,   0,
  "X1", -.15,   .25,
  "X2", -.35,   .35,
  "X3", -.55,   .45,
  "Y",  1,   0,
  "W", .5, -.25
)

dagify(Y ~ X1 + X2 + X3 + D,
       D ~ X1 + X2 + X3,
       coords = coords)%>% 
  tidy_dagitty() %>% 
  ggdag() + 
  theme_void()

```


---

## The hard part: satisfying conditional ignorability

In applied settings, can you observe the covariates that make conditional ignorability plausible? 

i.e. such that treatment is as-if random conditional on $\mathbf{X}_i$

i.e. such that the effect of $D_i$ on $Y_i$ is the only unblocked path connecting them?

--

For example

- if studying effect of democracy on economic growth, is it enough to control for past growth?
- if studying effect of attending private university on future earnings, is it enough to control for the exact set of universities to which the student was admitted? (example from Angrist & Pischke, *Mastering 'Metrics*)

--

Take causal inference.

???

This is why people end up doing experiments and looking for natural experiments, RDD, IV, diff-in-diff, etc.

---

## What should you control for?

From potential outcomes/ignorability standpoint or DAG standpoint, should be clear that you should control for variables that 

- affect the treatment, *and*
- affect the outcome

--

Don't control for other effects of the treatment ("post-treament variables").

---

## What should you control for? An OVB perspective

In seminars, we're often focused on a regression coefficient.

A common critique is "But you didn't control for $X$". 

--

A useful way to assess how (not) controlling for $X$ might affect a coefficient of interest: the omitted variable bias (OVB) formula. 

---

## The omitted bias formula

Consider three fitted regression equations: 

\begin{align*}
Y_i &= \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i + \hat{r}_i \qquad \text{(long eqn)} \\
Y_i &= \hat{\gamma}_0 + \hat{\gamma}_1 D_i + \hat{\varepsilon}_i \qquad \quad \text{(short eqn)} \\
X_i &= \hat{\alpha}_0 + \hat{\alpha}_1 D_i + \hat{u}_i \qquad \text{(auxiliary eqn)}
\end{align*}

OVB equation says: $\hat{\gamma}_1 = \hat{\beta}_1 + \hat{\beta}_2 \hat{\alpha}_1$.

**Shorthand:** "short" coefficient is "long" coefficient plus "impact" (of $X$ on $Y$) times  "imbalance" (in $X$ across $D$). 

--

**Proof**: Substitute auxiliary equation into long equation and simplify:

\begin{align*}
\hat{Y}_i &= \hat{\beta}_0 + \hat{\beta}_1 D_i  + \hat{\beta}_2 (\hat{\alpha}_0 + \hat{\alpha}_1 D_i + \hat{u}_i) + \hat{r}_i \\
&= \hat{\beta}_0 + \hat{\beta}_2 \hat{\alpha}_0 + (\hat{\beta}_1 +  \hat{\beta}_2 \hat{\alpha}_1 ) D_i + \hat{\beta}_2 \hat{u}_i + \hat{r}_i
\end{align*}

---

## Illustration of OVB formula

```{r}
pres <- read_csv("./../data/pres_data.csv") %>% filter(year < 2020)
long <- lm(incvote ~ juneapp + q2gdp, data = pres)
coef(long)
```
--
```{r}
short <- lm(incvote ~ juneapp, data = pres)
coef(short)
```
--
```{r}
auxiliary <- lm(q2gdp ~ juneapp, data = pres)
coef(auxiliary)
```
--
```{r}
coef(long)["juneapp"] + coef(long)["q2gdp"]*coef(auxiliary)["juneapp"] 
```

---
class: inverse, middle, center

## Regression with a single categorical confounder 

---

Setup: we assume three categories $X_i \in \{1,2,3\}$; the probability of treatment, the treatment effect, and the potential outcomes all depend on $X_i$.

First we compute the estimand ATE and the population regression estimate. Each can be seen as the weighted average of the CATEs. For the ATE the weights are the group weights. For the regression estimate the weights are the variance of treatment times the group weights. 

```{r}
taus <- c(1,2,3)
pDs <- c(1/2, 1/8, 7/8)
pXs <- c(1/4, 1/4, 1/2)
(ATE <- sum(pXs*taus))
(reg_estimate <- sum(pXs*pDs*(1-pDs)*taus)/sum(pXs*pDs*(1-pDs)))
```

---

Now we want to get the estimates for samples.

We make a function for drawing a dataset from parameters:

```{r}
draw_data <- function(n, pDs, pXs, taus){
  tibble(X = sample(x = c(1,2,3), size = n, replace = T, prob = pXs),
       D = rbinom(n, 1, pDs[X]),
       Y0 = X + rnorm(n),
       Y1 = Y0 + taus[X] + rnorm(n),
       Y_obs = ifelse(D == 1, Y1, Y0))
}
# an example dataset for illustration
set.seed(30500)
n <- 1000
(dat <- draw_data(n = n, pDs = pDs, pXs = pXs, taus = taus))
```


---

We start by computing the difference-in-means by $X$:

```{r}
diff_in_means_from_dat <- function(dat){
  dat %>% 
  group_by(X, D) %>% 
  summarize(mean(Y_obs), .groups = "drop") %>% 
  pivot_wider(names_from = D, 
              names_prefix = "D=", 
              values_from = `mean(Y_obs)`) %>% 
  mutate(`diff-in-means` = `D=1` - `D=0`)
}
(dims <- diff_in_means_from_dat(dat))
```

---

An unbiased estimator of the ATE weights these by prevalance of $X$ in the sample: 

```{r}
freqs_from_dat <- function(dat){
  dat %>% 
    count(X) %>% 
    mutate(freq = n/sum(n))
}

ate_from_dims_from_dat <- function(dat){
  dims <- diff_in_means_from_dat(dat)
  freqs <- freqs_from_dat(dat)
  freqs %>% 
    left_join(dims, by = "X") %>% 
    summarize(est = sum(freq*`diff-in-means`)) %>%
    as.numeric()
}
ate_from_dims_from_dat(dat)

```

---

The regression estimate weights the CATEs by frequency $\times$ variance of treatment:


```{r}
reg_est_from_variances_etc <- function(dat){
  dims <- diff_in_means_from_dat(dat)
  freqs <- freqs_from_dat(dat)
  dat %>% 
    group_by(X) %>% 
    summarize(varD = mean(D)*(1 - mean(D))) %>% 
    left_join(freqs) %>% 
    left_join(dims) %>% 
    summarize(reg_est = sum(freq*varD*`diff-in-means`)/sum(freq*varD)) %>% 
    as.numeric()
}

reg_est_from_variances_etc(dat)
reg_est_from_lm <- function(dat){
  lm(Y_obs ~ D + factor(X), data = dat)$coefficients["D"]  
}
reg_est_from_lm(dat)
```

---

Now we do multiple samples.

```{r}
J <- 500
sims <- tibble(j = 1:J,
       n = n,
       pXs = list(pXs),
       taus = list(taus),
       pDs = list(pDs)) %>% 
  select(-j) %>% 
  mutate(dat = pmap(., draw_data))

sims %>% 
  mutate(reg_ests = map_dbl(dat, reg_est_from_lm),
         ate_ests = map_dbl(dat, ate_from_dims_from_dat)) -> ests
```

---

class: small-output

And we look at the results: 

```{r, out.width = "80%", out.height = "50%"}
ests %>% 
  select(reg_ests, ate_ests) %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value, fill = name)) + 
  geom_density(alpha = .5) + 
  geom_vline(xintercept = reg_estimate, col = "blue") + 
  geom_vline(xintercept = ATE, col = "red") + 
  scale_fill_manual(values = c("red", "blue"), 
                    labels = c("Matching", "Regression")) + 
  annotate("text", x = c(reg_estimate, ATE) + c(-.05, .05), y = c(3,3.5), label = c("Analytical\nregression\nestimate", "ATE"), hjust = c(1,0)) + 
  labs(fill = "", x = "Estimate", y = "")
  
```


---

Now, we can recover the weighted-CATE-estimates estimate of the ATE by running the regression with interactions between treatment and $X$, getting fitted values for both potential outcomes, and computing the differences in the average potential outcomes: 

```{r}
reg_estimator_w_intx <- function(dat){
  mod <- lm(Y_obs ~ D*factor(X), data = dat)
  mean(predict(mod, newdata = dat %>% mutate(D = 1))) - mean(predict(mod, newdata = dat %>% mutate(D = 0))) 
}

reg_estimator_w_intx(dat)
ate_from_dims_from_dat(dat)
```

---

So what have we shown? 

- an unbiased estimator of the ATE, given strong ignorability, computes the difference in means at each level of $X$ and combines them by the prevalence of $X$ 
- the regression of $Y$ on $D$ with categorical dummies ("fixed effects") is not unbiased for the ATE: it yields an estimate equal to the differences-in-means weighted by prevalence of $X$ $\times$ the conditional variance of $D$
- you can use regression to recover the unbiased estimator of the ATE by including interactions between $D$ and $X$, imputing both potential outcomes from the regression model, and taking the difference in average potential outcomes.

People don't do the latter because it's a little annoying and requires the bootstrap for standard errors. More cynically, because they don't understand this, don't have a specific estimand in mind, and don't care about the point estimate of the ATE (just its direction and significance), perhaps because the concepts/measurements/units are all unclear.


